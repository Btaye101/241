---
title: "hmrk 7 Beimnet Taye"
output: pdf_document
date: "2023-03-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(simcausal)
library(haven)
library(here)
```
# Worked with Joan Shim and Lucas Yoshida

## P1

### 1

-   The input for the estimand is the underlying total population outputs of a given RV. The output is a measure of the RV values from the total population inputted (mean, median, etc).

### 2

-   The input for the estimator is the sampled data of a given RV and the output is a measure of said data (mean, median, etc.) which is an estimate of the estimand.

### 3

-   The estimand is the true value of a measure in a population. The estimator tries to estimate the estimand using sample data.

### 4.

#### Bias and Variance:

-   $Bias = \mu(\hat{\psi}) - \psi$

-   $= 2 - 5$

-   $= -3$

-   $Var(\hat{\psi}) = E[\hat{\psi}^2] - E[\hat{\psi}]^2$

-   $= 4-4$

-   $= 0$

## P2

### 3

```{r}
dgp = function(n) {
tibble(X = rnorm(n)) # your code here
}
sigma_a2_estimator = function(data) {
n = nrow(data)
data %$% sum((X - mean(X))^2) / n
}
sigma_b2_estimator = function(data) {
n = nrow(data)
data %$% sum((X - mean(X))^2) / (n-1)
}

bias_raw <- function(dgp,n,rep =1000){
    map_df(1:rep, function(.x){
      obs <- dgp(n)
      return(
        tibble(
         sample_size = n,
         alpha = sigma_a2_estimator(obs),
         beta = sigma_b2_estimator(obs)
             )
        )
      }
    )
}

bias_eval <- function(dgp,n){
  bias_raw(dgp,n) %>%
    mutate(estimand = var(dgp(100000000)$X)) %>% 
    summarize(alpha_m = mean(alpha),
              beta_m = mean(beta),
              true_var = mean(estimand),
              sample_size = mean(n),
              bias_a = alpha_m - true_var,
              bias_b = beta_m - true_var
              )
  
}

small <- bias_eval(dgp,10)
small

large <- bias_eval(dgp,100000)
large

small_densities <- bias_raw(dgp,10) %>% 
  ggplot() +
  geom_density(aes(x = alpha), color = "blue") +
  geom_density(aes(x = beta), color = "red") +
  geom_vline(xintercept = var(dgp(10000000)$X), color = "green") +
  geom_vline(aes(xintercept = mean(alpha)), color = "blue") +
  geom_vline(aes(xintercept = mean(beta)), color = "red")
small_densities


large_densities <- bias_raw(dgp,100000) %>% 
  ggplot() +
  geom_density(aes(x = alpha), color = "blue") +
  geom_density(aes(x = beta), color = "red") +
  geom_vline(xintercept = var(dgp(10000000)$X), color = "green") +
  geom_vline(aes(xintercept = mean(alpha)), color = "blue") +
  geom_vline(aes(xintercept = mean(beta)), color = "red")
large_densities



```

-   Based off the calculated tables the bias is larger with the alpha method when the sample size is small and close to 0 when the sample is larger. With the generated densities the mean for the alpha method is farther away from the estimand value of one for the smaller sample size.

### 4.

-   The analytic proof is more robust and grounded in raw theory but is much harder to parse through. The computational method is a lot more readable and intuitive but scarifies some robustness.

### 5.

-   The beta method while less biased than the alpha method actually has a higher amount of variance. This is called the bias-variance tradeoff and might be a reason you might want to use the alpha method instead. 

## P3

### 1.

```{r}
logistic <- function(x){
  1/(1+exp(-x))
}
DGP_o <- function(n){tibble(
  X = runif(n),
  W = rbern(n, prob = logistic(X)),
  Y = rbern(n, prob = logistic(X+W))
)
}

data <- DGP_o(10000) %>% 
  group_by(W) %>% 
  summarize(one_probs = mean(Y),
            zero_probs = 1 - one_probs,
            odds = one_probs/zero_probs) 
odds_ratio <- pull(data[2,4])/pull(data[1,4])
odds_ratio

              
            
```

## P4

### 1.

```{r}
dgp_components = list(
covariates = function(n) {
tibble(
X1 = rnorm(n),
X2 = rnorm(n),
X3 = rnorm(n),
  )
},

response = function(data) {
n = nrow(data)
data %>%
mutate(
D = rbern(n, logistic(-X1 - 2*X2 + 3*X3)),
  )
},

opiates = function(data) {
n = nrow(data)
data %>%
mutate(
Y = rbern(n, logistic(X1 + X2 - 2*X3 - 3)),
  )
}
)

true_dgp = function(n) {
dgp_components %$% {
covariates(n) %>%
response %>%
opiates
  }
}

observed_dgp = function(n) {
true_dgp(n) %>%
mutate(Y = ifelse(D, Y, NA))
}


```

```{r}
Estimand_Y <- mean(true_dgp(100000)$Y)
Estimand_Y 
```

### 2.

```{r}

unadjusted = function(data){
mean(data$Y, na.rm=T)
}

regression = function(data){
data %>%
filter(!is.na(Y)) %>%
glm(Y ~ X1+X2+X3, data=., family=binomial) %>%
predict(data, type='response') %>%
mean()
}

weighted = function(data){
data %>%
glm(D ~ X1+X2+X3, data=., family=binomial) %>%
predict(data, type='response') %>%
mutate(data, p=.) %>%
filter(!is.na(Y)) %$%
    { sum(Y/p) / nrow(data) }
  
}

```

```{r}
observed <- observed_dgp(500)

estimates <- tibble(
  unadjusted = unadjusted(observed),
  regression = regression(observed),
  weighted = weighted(observed)
)
estimates
```

### 3.

```{r}
bias_rawd <- function(dgp,n=500,rep=10){
    map_df(1:rep, function(.x){
      obs <- dgp(n)
      return(
        tibble(
         sample_size = n,
         unadjusted = unadjusted(obs),
         regression = regression(obs),
         weighted = weighted(obs)
             )
        )
      }
    )
}

bias_evald <- function(dgp,true_dgp,n=500,rep=10) {
  bias_rawd(dgp, n=500,rep=10) %>%
    mutate(estimand = mean(true_dgp(1000000)$Y)) %>% 
    summarize(bias_unadusted = mean(unadjusted)-mean(estimand),
              var_unadjusted = var(unadjusted),
              bias_regression =  mean(regression)-mean(estimand), 
              var_regression = var(regression),
              bias_weighted = mean(unadjusted) - mean(estimand),
              var_weighted = var(weighted))
}
result <- bias_evald(observed_dgp,true_dgp, rep = 10000)
final <- tibble(Estimator = c("unadjusted","regression","weighted"),
                bias = as.numeric(c(result[1,1],result[1,3],result[1,5])),
                variance = as.numeric(c(result[1,2],result[1,4],result[1,6]))
                )
final



```

#### 4.

-   I would use the regression estimator since it has the lowest amount of bias by a fairly large margin relative to the other two estimators. Yes it would since estimators in general are dependent on the underlying DGP. The best estimator we choose is usually based upon assumptions we make about the underlying distribution, such as how the underlying population data is distributed. If the DGP were to change our assumptions used to choose an estimator would likely no longer hold and we would have to make adjustments.

## P5

### 1.

-   $\sigma^2 = V[\hat{\mu}]$
-   $= V[\frac{1}{n}\sum_{i}X_i]$
-   $=\frac{1}{n^2}V[\sum_{i}X_i]$ Linearity of variance.
-   $=\frac{1}{n^2}\sum_{i}V[X_i]$
-   $=\frac{1}{n^2}V[X]*n$ The sum of the variances of an IID RV is the just the variance of the sum.
-   $=\frac{V[X]n}{n^2}$
-   $=\frac{V[X]}{n}$

### 2.

-   We can arrive at this since variance is defined as:
-   $V[\hat{X}] = E[(\hat{X}-E[\hat{X}])^2]$
-   Using the above equation we can use the plug in estimators for expectation:
-   $E[\hat{X}] = \frac{1}{n}\sum_{i}X_i$
-   $V[\hat{X}] = \frac{1}{n}\sum_{i}(\hat{X}-(\frac{1}{n}\sum_{i}X_i))^2$
-   Given:
-   $\hat{\mu} = \frac{1}{n}\sum_{i}X_i$
-   We can plug this in and we get:
-   $V[\hat{X}]=\frac{1}{n}\sum_{i}(\hat{X}-\hat{\mu})^2$
-   We can use the plug in estimator for the variance,derived in the previous question, into the equation:
-   $\hat{\sigma^2}= \frac{V[\hat{X}]}{n}$
-   $= \frac{\frac{1}{n}\sum_{i}(\hat{X}-\hat{\mu})^2}{n}$
-   $= \frac{1}{n^2}\sum_{i}(\hat{X}-\hat{\mu})^2$
-   Finally the Standard deviation of the sampling distribution, called the standard error, is just the square root of its variance:
-   $\sigma = \sqrt{\frac{1}{n^2}\sum_{i}(\hat{X}-\hat{\mu})^2}$

### 3.

```{r}
# Diff distributions
dgp_5 <- function(n) {
  return(rnorm(n))
}
dgp_6 <- function(n) {
  return(rexp(n))
}
dgp_7 <- function(n){
  return(runif(n))
}
#STE Calc
STE <- function(dgp,n) {
  sigma <- sqrt((1/n^2)*sum((dgp-mean(dgp))^2))
  return(sigma)
}
#table with interval values
interval <- function(dgp,n, rep=100) {
  map_df(1:rep,function(.x){
      data <- dgp(n)
      Sig <- STE(data,n)  
      return(
        tibble(
        mu = mean(data),
        upper = mu + 2*Sig,
        lower = mu - 2*Sig,
        sample_size = n
        )
      )
    }
  ) %>% 
    mutate(row = row_number()*5)
}
# interval visualization
int_visual <- function(dgp,n,rep=100){
  interval(dgp,n,rep) %>% 
  ggplot() + 
  geom_segment(aes(x = lower, xend = upper, y = row, yend = row)) +
  geom_vline(xintercept = mean(dgp(1000000)), color = "green")
}

# normal 
# n = 100
normal_table_100 <- interval(dgp_5,100)
normal_table_100
normal_visual_100 <- int_visual(dgp_5,100)
normal_visual_100

# n = 500
normal_table_500 <- interval(dgp_5,500)
normal_table_500
normal_visual_500 <- int_visual(dgp_5,500)
normal_visual_500
# n = 1000
normal_table_1000 <- interval(dgp_5,1000)
normal_table_1000
normal_visual_1000 <- int_visual(dgp_5,1000)
normal_visual_1000
# n = 5000
normal_table_5000 <- interval(dgp_5,5000)
normal_table_5000
normal_visual_5000 <- int_visual(dgp_5,5000)
normal_visual_5000
# n = 10000
normal_table_10000 <- interval(dgp_5,10000)
normal_table_10000
normal_visual_10000 <- int_visual(dgp_5,10000)
normal_visual_10000

# exponential
# n = 100
exp_table_100 <- interval(dgp_6,100)
exp_table_100
exp_visual_100 <- int_visual(dgp_6,100)
exp_visual_100

# n = 500
exp_table_500 <- interval(dgp_6,500)
exp_table_500
exp_visual_500 <- int_visual(dgp_6,500)
exp_visual_500
# n = 1000
exp_table_1000 <- interval(dgp_6,1000)
exp_table_1000
exp_visual_1000 <- int_visual(dgp_6,1000)
exp_visual_1000
# n = 5000
exp_table_5000 <- interval(dgp_6,5000)
exp_table_5000
exp_visual_5000 <- int_visual(dgp_6,5000)
exp_visual_5000
# n = 10000
exp_table_10000 <- interval(dgp_6,10000)
exp_table_10000
exp_visual_10000 <- int_visual(dgp_6,10000)
exp_visual_10000

# uniform
# n = 100
uni_table_100 <- interval(dgp_7,100)
uni_table_100
uni_visual_100 <- int_visual(dgp_7,100)
uni_visual_100

# n = 500
uni_table_500 <- interval(dgp_7,500)
uni_table_500
uni_visual_500 <- int_visual(dgp_7,500)
uni_visual_500
# n = 1000
uni_table_1000 <- interval(dgp_7,1000)
uni_table_1000
uni_visual_1000 <- int_visual(dgp_7,1000)
uni_visual_1000
# n = 5000
uni_table_5000 <- interval(dgp_7,5000)
uni_table_5000
uni_visual_5000 <- int_visual(dgp_7,5000)
uni_visual_5000
# n = 10000
uni_table_10000 <- interval(dgp_7,10000)
uni_table_10000
uni_visual_10000 <- int_visual(dgp_7,10000)
uni_visual_10000
```

### 4.

```{r}
data = read_xpt("CDQ_H.XPT") # replace w/ appropriate file path
prev_chest <- mean(data$CDQ001)
prev<- data %>% 
  count(CDQ001) %>% 
  mutate(prev = n/sum(n))
prev_chest <- pull(prev[1,3])
prev_chest
```

### 5.

-   You can try and calculate a 95% confidence interval to see if .3 is contained in the interval. If it is not then you can try to use that to say that the 30% measured is probably not the true value. A caveat to this argument is that by random chance (5% with 95% CI) our sample can generate an interval that does not contain the true value which in that case can in fact be 30%.
