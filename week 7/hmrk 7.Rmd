---
title: "hmrk 7 Beimnet Taye"
output: pdf_document
date: "2023-03-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(simcausal)
```

## P1

### 1
-   The input for the estimand is the underlying total population outputs of a given RV. The output is a measure of the RV values from the total population inputted (mean, median, etc).

### 2
-   The input for the estimator is the sampled data of a given RV and the output is a measure of said data (mean, median, etc.) which is an estimate of the estimand.

### 3
-   The estimand is the true value of a measure in a population. The estimator tries to estimate the estimand using sample data.

### 4.
#### Bias:
$Bias = \mu(\hat{\psi}) - \psi$
$ = 2 - 5$
$ = -3$
#### Variance:
$Var(\hat{\psi}) = E[\hat{\psi}^2] - E[\hat{\psi}]^2$
$= 4-4$
$= 0$


## P2

### 3

```{r}
dgp = function(n) {
tibble(X = rnorm(n)) # your code here
}
sigma_a2_estimator = function(data) {
n = nrow(data)
data %$% sum((X - mean(X))^2) / n
}
sigma_b2_estimator = function(data) {
n = nrow(data)
data %$% sum((X - mean(X))^2) / (n-1)
}

bias_raw <- function(dgp,n,rep =1000){
    map_df(1:rep, function(.x){
      obs <- dgp(n)
      return(
        tibble(
         sample_size = n,
         alpha = sigma_a2_estimator(obs),
         beta = sigma_b2_estimator(obs)
             )
        )
      }
    )
}

bias_eval <- function(dgp,n){
  bias_raw(dgp,n) %>%
    mutate(estimand = var(dgp(100000000)$X)) %>% 
    summarize(alpha_m = mean(alpha),
              beta_m = mean(beta),
              true_var = mean(estimand),
              sample_size = mean(n),
              bias_a = alpha_m - true_var,
              bias_b = beta_m - true_var
              )
  
}

test <- bias_raw(dgp,10)
small <- bias_eval(dgp,10)
small

test
large <- bias_eval(dgp,100000)
large

small_densities <- bias_raw(dgp,10) %>% 
  ggplot() +
  geom_density(aes(x = alpha), color = "blue") +
  geom_density(aes(x = beta), color = "red") +
  geom_vline(xintercept = var(dgp(10000000)$X), color = "green") +
  geom_vline(aes(xintercept = mean(alpha)), color = "blue") +
  geom_vline(aes(xintercept = mean(beta)), color = "red")
small_densities


large_densities <- bias_raw(dgp,100000) %>% 
  ggplot() +
  geom_density(aes(x = alpha), color = "blue") +
  geom_density(aes(x = beta), color = "red") +
  geom_vline(xintercept = var(dgp(10000000)$X), color = "green") +
  geom_vline(aes(xintercept = mean(alpha)), color = "blue") +
  geom_vline(aes(xintercept = mean(beta)), color = "red")
large_densities



```

-   Based off the calculated tables the bias is larger with the alpha method when the sample size is small and close to 0 when the sample is larger. With the generated densities the mean for the alpha method is farther away from the expected true value of one for the smaller sample size.

### 4.
-   The analytic proof is more robust and grounded in raw theory but is much harder to parse. The computational method is a lot more readable and intuitive but scarifies some robustness. 

### 5.
-   bias variance tradeoff. Beta method trades less bias for more variance.


## P3

### 1.
```{r}
logistic <- function(x){
  1/(1+exp(-x))
}
DGP_o <- function(n){tibble(
  X = runif(n),
  W = rbern(n, prob = logistic(X)),
  Y = rbern(n, prob = logistic(X+W))
)
}

data <- DGP_o(10000) %>% 
  group_by(W) %>% 
  summarize(one_probs = mean(Y),
            zero_probs = 1 - one_probs,
            odds = one_probs/zero_probs) 
odds_ratio <- pull(data[2,4])/pull(data[1,4])
odds_ratio

              
            
```


## P4

### 1.

```{r}
dgp_components = list(
covariates = function(n) {
tibble(
X1 = rnorm(n),
X2 = rnorm(n),
X3 = rnorm(n),
  )
},

response = function(data) {
n = nrow(data)
data %>%
mutate(
D = rbern(n, logistic(-X1 - 2*X2 + 3*X3)),
  )
},

opiates = function(data) {
n = nrow(data)
data %>%
mutate(
Y = rbern(n, logistic(X1 + X2 - 2*X3 - 3)),
  )
}
)

true_dgp = function(n) {
dgp_components %$% {
covariates(n) %>%
response %>%
opiates
  }
}

observed_dgp = function(n) {
true_dgp(n) %>%
mutate(Y = ifelse(D, Y, NA))
}


```

```{r}
Estimand_Y <- mean(true_dgp(100000)$Y)
Estimand_Y 
```

### 2.
```{r}

unadjusted = function(data){
mean(data$Y, na.rm=T)
}

regression = function(data){
data %>%
filter(!is.na(Y)) %>%
glm(Y ~ X1+X2+X3, data=., family=binomial) %>%
predict(data, type='response') %>%
mean()
}

weighted = function(data){
data %>%
glm(D ~ X1+X2+X3, data=., family=binomial) %>%
predict(data, type='response') %>%
mutate(data, p=.) %>%
filter(!is.na(Y)) %$%
    { sum(Y/p) / nrow(data) }
  
}

```

```{r}
observed <- observed_dgp(500)

estimates <- tibble(
  unadjusted = unadjusted(observed),
  regression = regression(observed),
  weighted = weighted(observed)
)
estimates
```

### 3.
```{r}
bias_rawd <- function(dgp,n=500,rep=10){
    map_df(1:rep, function(.x){
      obs <- dgp(n)
      return(
        tibble(
         sample_size = n,
         unadjusted = unadjusted(obs),
         regression = regression(obs),
         weighted = weighted(obs)
             )
        )
      }
    )
}

bias_evald <- function(dgp,true_dgp,n=500,rep=10) {
  bias_rawd(dgp, n=500,rep=10) %>%
    mutate(estimand = mean(true_dgp(1000000)$Y)) %>% 
    summarize(bias_unadusted = mean(unadjusted)-mean(estimand),
              var_unadjusted = var(unadjusted),
              bias_regression =  mean(regression)-mean(estimand), 
              var_regression = var(regression),
              bias_weighted = mean(unadjusted) - mean(estimand),
              var_weighted = var(weighted))
}
result <- bias_evald(observed_dgp,true_dgp, rep = 10000)
final <- tibble(Estimator = c("unadjusted","regression","weighted"),
                bias = as.numeric(c(result[1,1],result[1,3],result[1,5])),
                variance = as.numeric(c(result[1,2],result[1,4],result[1,6]))
                )
final



```

#### 4.

I would use the regression estimator since it has the lowest amount of bias by a fairly large margin relative to the other two estimators. Yes it would since